{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef802643-02f8-423f-bdac-abb885852e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1ac18e-4010-4a70-9e73-bc9bb80541e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# 1. Класс для сбора метрик\n",
    "# =============================\n",
    "class MetricsCollector:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.token_times = []\n",
    "        self.tokens = []\n",
    "\n",
    "    def callback(self, token: int) -> bool:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - self.start_time\n",
    "        self.token_times.append(elapsed)\n",
    "        self.tokens.append(token)\n",
    "        # Прерываем генерацию, если встретились определённые токены (пример условия)\n",
    "        return token in [151643, 151645]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce6b112f-27cd-4010-a5df-6a7ce9f6d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. Тест для кастомной модели (с/без кэша)\n",
    "# ==========================================\n",
    "def test_custom(use_cache: bool):\n",
    "    # Импорт кастомной модели (предполагается, что Processor реализован)\n",
    "    from models.processor import Processor\n",
    "\n",
    "    metrics_collector = MetricsCollector()\n",
    "    messages = [{\"role\": \"system\", \"content\": \"Расскажи про Олега Гербылева?\"}]\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_before = process.memory_info().rss\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Инициализируем модель\n",
    "    processor = Processor(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "    # Генерируем ответ с указанным флагом кэширования\n",
    "    result = processor(messages, custom_callback=metrics_collector.callback, use_cache=use_cache, max_new_tokens = 500)\n",
    "    end_time = time.time()\n",
    "\n",
    "    mem_after = process.memory_info().rss\n",
    "    elapsed = end_time - start_time\n",
    "    mem_used = (mem_after - mem_before) / (1024 * 1024)  # в МБ\n",
    "    tokens_count = len(metrics_collector.tokens)\n",
    "\n",
    "    # Очистка памяти после работы модели\n",
    "    del processor\n",
    "    gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"model\": \"custom\",\n",
    "        \"use_cache\": use_cache,\n",
    "        \"time\": elapsed,\n",
    "        \"memory\": mem_used,\n",
    "        \"tokens\": tokens_count,\n",
    "        \"response\": result[0] if result else \"\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd834e1-84fb-46a2-aaf4-ae5e29c15ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# 3. Тест для оригинальной модели (Transformers)\n",
    "# ====================================================\n",
    "def test_transformers(use_cache: bool):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    model_name = \"Qwen/Qwen2-0.5B-Instruct\"  # можно заменить на другую модель\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Устанавливаем флаг кэширования (если модель поддерживает)\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    prompt = \"Расскажи про Олега Гербылева?\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    import psutil, os, time, gc\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_before = process.memory_info().rss\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Гарантированная генерация ровно 500 новых токенов (без учета токенов промпта)\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        min_new_tokens=500,\n",
    "        max_new_tokens=500,\n",
    "        do_sample=True  # или False – зависит от желаемого стиля генерации\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    mem_after = process.memory_info().rss\n",
    "    elapsed = end_time - start_time\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    tokens_count = len(tokenizer.tokenize(output_text))\n",
    "\n",
    "    # Очистка памяти после работы модели\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"model\": \"original\",\n",
    "        \"use_cache\": use_cache,\n",
    "        \"time\": elapsed,\n",
    "        \"memory\": (mem_after - mem_before) / (1024 * 1024),  # в МБ\n",
    "        \"tokens\": tokens_count,\n",
    "        \"response\": output_text\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bda0717-56f9-4266-bfb5-1b94f74f46db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dd753f300c4ec9bfaa53d4a36d7ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 5.79 GiB of which 20.31 MiB is free. Process 9605 has 3.94 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 1.37 GiB is allocated by PyTorch, and 88.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 4. Запуск 4-х тестов и сбор результатов\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m      4\u001b[39m results = []\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results.append(\u001b[43mtest_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m      6\u001b[39m results.append(test_custom(use_cache=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m      7\u001b[39m results.append(test_transformers(use_cache=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtest_custom\u001b[39m\u001b[34m(use_cache)\u001b[39m\n\u001b[32m     15\u001b[39m processor = Processor(\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2-0.5B-Instruct\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Генерируем ответ с указанным флагом кэширования\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics_collector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m end_time = time.time()\n\u001b[32m     20\u001b[39m mem_after = process.memory_info().rss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI-experements/Qwen-from-scratch/models/processor.py:31\u001b[39m, in \u001b[36mProcessor.__call__\u001b[39m\u001b[34m(self, inputs, device, custom_callback, use_cache, max_new_tokens)\u001b[39m\n\u001b[32m     29\u001b[39m device = device \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m input_ids = \u001b[38;5;28mself\u001b[39m.encode(prompt, device)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI-experements/Qwen-from-scratch/models/processor.py:37\u001b[39m, in \u001b[36mProcessor.generate\u001b[39m\u001b[34m(self, input_ids, custom_callback, use_cache, max_new_tokens)\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token == \u001b[32m151643\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m token == \u001b[32m151645\u001b[39m\n\u001b[32m     36\u001b[39m inpt_len = input_ids.size(\u001b[32m1\u001b[39m) + \u001b[32m3\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcustom_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcustom_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decode(output[: , inpt_len:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI-experements/Qwen-from-scratch/models/model.py:203\u001b[39m, in \u001b[36mQwen2.generate\u001b[39m\u001b[34m(self, input_ids, callback, max_new_tokens, use_cache)\u001b[39m\n\u001b[32m    201\u001b[39m x = \u001b[38;5;28mself\u001b[39m.model.norm(x)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lm_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     logits = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 5.79 GiB of which 20.31 MiB is free. Process 9605 has 3.94 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 1.37 GiB is allocated by PyTorch, and 88.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. Запуск 4-х тестов и сбор результатов\n",
    "# ==========================================\n",
    "results = []\n",
    "results.append(test_custom(use_cache=True))\n",
    "results.append(test_custom(use_cache=False))\n",
    "results.append(test_transformers(use_cache=True))\n",
    "# results.append(test_transformers(use_cache=False))\n",
    "\n",
    "# Вывод результатов в консоль\n",
    "for res in results:\n",
    "    model_type = \"Кастомная\" if res[\"model\"] == \"custom\" else \"Оригинальная\"\n",
    "    cache_state = \"с кэшем\" if res[\"use_cache\"] else \"без кэша\"\n",
    "    print(f\"{model_type} модель ({cache_state}):\")\n",
    "    print(f\"  Время генерации: {res['time']:.2f} сек\")\n",
    "    print(f\"  Потребление памяти: {res['memory']:.2f} МБ\")\n",
    "    print(f\"  Количество токенов: {res['tokens']}\")\n",
    "    print(f\"  Ответ: {res['response']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02296289-0875-4ab3-8b47-11e21f8d8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5. Построение графиков для сравнения\n",
    "# ==========================================\n",
    "# Подготавливаем метки тестов\n",
    "labels = [\n",
    "    \"Custom (cache)\",\n",
    "    \"Custom (no cache)\",\n",
    "    \"Original (cache)\",\n",
    "    # \"Original (no cache)\"\n",
    "]\n",
    "times = [res[\"time\"] for res in results]\n",
    "memories = [res[\"memory\"] for res in results]\n",
    "tokens = [res[\"tokens\"] for res in results]\n",
    "\n",
    "# График: Время генерации\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, times, color='skyblue')\n",
    "plt.ylabel(\"Время генерации (сек)\")\n",
    "plt.title(\"Сравнение времени генерации\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# График: Потребление памяти\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, memories, color='salmon')\n",
    "plt.ylabel(\"Потребление памяти (МБ)\")\n",
    "plt.title(\"Сравнение использования памяти\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# График: Количество токенов\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, tokens, color='lightgreen')\n",
    "plt.ylabel(\"Количество токенов\")\n",
    "plt.title(\"Сравнение объёма сгенерированных токенов\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5bfa8-1036-402b-85e5-2781147807ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
